% Dependency between concepts
% -> materials science 
% -> Materials informatics 
% -> superconductivity 
% -> SuperCon
% -> Computational science 

% General introduction: 
% -> TDM 
% -> ML

% Materials science 
Materials science is a multidisciplinary field at the intersection of physics, chemistry, and engineering, dedicated to understanding, designing, and manipulating materials for a myriad of real applications. 
Central to this discipline is the exploration of the structure, properties, and performance of materials, ranging from metals and ceramics to polymers and composites. By delving into the microscopic and atomic scales, materials scientists seek to uncover the fundamental principles that govern a material's behaviour and tailor its properties to meet specific technological needs. 
This field plays a critical role in advancing technology and innovation, as discoveries in materials science often lead to the development of new materials with enhanced functionalities, improved performance, and novel applications across industries.

% Materials informatics 
Historically, breakthroughs in materials science often resulted from chance discoveries and unexpected observations, a process driven by serendipity and relying on trial and error. 
However, with the advent of advanced computational tools, machine learning, and big data analytics, materials informatics (MI) has emerged as a transformative methodology. 
Materials informatics is a multidisciplinary field that leverages computational methods, data science, and informatics techniques to accelerate the discovery and design of new materials, identify patterns, and predict material properties with unprecedented accuracy. 
Materials informatics not only expedites the discovery of novel materials but also enhances our understanding of complex relationships between structure, composition, and performance. 
This approach allows for a more efficient and systematic exploration of the vast material space, potentially leading to breakthroughs in fields such as electronics, energy, healthcare, and beyond. 

% Look at other fields 
In comparison of MI with well-established fields like computational chemistry and biology, we can discern not only the unique contributions of each discipline but also the collective progression towards harnessing computational power for transformative advancements in science and technology.
Computational chemistry is the earliest among the three, with its roots tracing back to the mid-20th century. The development of computational chemistry gained momentum with the advent of digital computers, and it has evolved significantly over the decades. Computational chemistry involves the application of theoretical methods and simulations to study the structure, properties, and behaviour of molecules, making it a well-established and mature field.
Computational biology, on the other hand, gained prominence later, particularly with the explosion of biological data in the post-genomic era. The field began to flourish in the late twentieth century and has since grown rapidly. Computational biology encompasses a broad range of techniques, including bioinformatics, molecular dynamics simulations, and systems biology, to model and analyse biological processes at various levels of complexity.
Materials informatics is a more recent entrant compared to computational chemistry and biology. Although the idea of using informatics approaches for material discovery has been around for some time, the field has gained significant traction over the past few decades, especially with the rise of advanced computing capabilities and the availability of large materials databases. 

Materials Informatics (MI) is founded on two key techniques: Density Functional Theory (DFT) computations and a data-driven approach using Machine Learning (ML). DFT computations, a core aspect of quantum mechanics simulations, delve into a material's electronic structure and properties at the atomic and molecular levels, providing a precise understanding that complements experimental data. On the other front, the data-driven approach utilises ML algorithms, from regression models to neural networks, for the analysis of extensive materials datasets. 
These algorithms decode patterns and correlations, enabling researchers to predict the properties of new materials or optimise existing ones. 
This combined approach, which integrates quantum-level insights with data-driven predictive modelling, accelerates the materials discovery process, facilitating the efficient exploration of diverse materials spaces.

Data-driven methods have significantly contributed to the design of new materials across various sub-domains, such as magneto-caloric, thermoelectrics, and superconductor materials. Despite the impactful role of these methods, a notable challenge lies in the limited availability of experimental datasets. 
Currently, the primary sources for such datasets in inorganic materials are Pauling File~\cite{Blokhin2018ThePF_paulingFile}, Starrydata~\cite{katsura2019data}, and SuperCon~\cite{ishii2023structuring}. 
The use of these databases highlights the need to broaden and diversify experimental data, improving the effectiveness and applicability of data-driven approaches in materials design. The addition of these datasets is crucial to advance the field, promote innovation, and expand the scope of materials discovery through data-driven methodologies.
SuperCon is the standard database for superconductor materials and it has been developed and manually maintained by NIMS since 1987. 
It consists of records of materials reported in scientific experiments and a large set of measured properties. 

A superconductor is a material that, when cooled below a critical temperature, exhibits zero electrical resistance and the expulsion of magnetic fields. This phenomenon, known as superconductivity, occurs in a variety of materials, with many of them requiring extremely low temperatures, often close to absolute zero, to manifest this unique behaviour. The critical temperature at which superconductivity emerges varies depending on the material. Superconductors have numerous applications, notably in the field of electrical engineering. They are utilised to create powerful electromagnets for applications such as magnetic resonance imaging (MRI) machines, particle accelerators, and magnetic levitation systems. Superconductors also play a crucial role in the development of high-speed, energy-efficient power transmission lines, since they can carry large currents without any loss. The potential for quantum computing and more efficient electronic devices is another area where superconductors are actively being explored, showcasing the interdisciplinary nature of their applications in physics, materials science, and technology.

SuperCon hosts about 32,000 materials records and a complex schema with about 200 properties, some of which have not been consistently collected~\cite{sommer20223dsc}. 
For example, the pressure applied to obtain superconductivity was reported in only 16 records; however, its studies and experiments date back to the 1970s, but it became more popular only at the beginning of the 21st century.  
However, SuperCon has been widely recognised for its quality and has been the data source for many attempts to design models that can predict Tc~\cite{stanev_machine_2017, le2020critical, Hamlin2019SuperconductivityNR}. 

However, using ML to predict Tc has some criticality that needs to be considered. 
The models have an intrinsic applicability domain, which means that predictions are limited to the patterns and trends encountered in the training set. 
This can lead to significant selection bias, making the models ineffective when applied to materials constituted by different compounds (or belonging to different classes). 
The variety of training data to be used should contain a relevant amount of non-superconductors materials to avoid training a model biased toward the assumption that a Tc always exists and render it ineffective when applied to new materials.
Now, the definition of non-superconductivity is tricky: if no superconducting critical temperature has been found, does not mean that one does not exist. It might be outside the range of current technological instruments. 
This leads to a conundrum when delving into the data: ignoring compounds with no reported Tc and maybe disregarding a potentially important part of the dataset for the sake of simplicity.
Simplifying too much could lead to an incomplete understanding of the factors that determine superconductivity and limit the ability to predict Tc accurately. 
Generally, in addition to these considerations, several SuperCon-related works attempted to integrate complementary data from other datasets. 
However, at the time of writing, only~\cite{sommer20223dsc} has aimed to directly enhance SuperCon.  
They successfully enhanced 9150 records with crystal structures from experimental results stored in the Inorganic Crystal Structure Database (ICSD). 
This recent work indicates that the SuperCon dataset continues to play a pivotal role in the landscape of superconductor research.

\section{Motivation}

The number of publications in superconductor research remained stable in the last 10 years\footnote{analysed on arXiv statistics on cond-mat category \url{https://info.arxiv.org/about/reports/submission_category_by_year.html}}.
It is reasonable to expect that future breakthroughs in superconductor research will be achieved through materials informatics, given its growing importance, and an updated SuperCon is to be considered a necessary condition. 

Two main challenges that SuperCon is facing motivate our work. 
The manual approach employed for compiling SuperCon is proving to be increasingly demanding in keeping up with the influx of new publications on time. The reliance on manual labour for dataset curation becomes a growing obstacle, as it demands specialised skills that are not readily available in the market. As the field of superconductor research evolves, the need for more efficient and scalable methods becomes apparent.
An automatic process is necessary to streamline and improve the population of SuperCon. 
The outline of the process comprises the ingestion of entire documents or text and the identification of materials and relative proprieties.  
In a small-scale preliminary assessment, we analysed the problem, identified several challenges to overcome, and proposed a data extraction framework~\cite{foppiano2019proposal}.

The expressions that identify materials are complex and lengthy and their identification in the scientific literature poses several challenges.
Furthermore, the strict definition of what a material cannot be expressed in a synthetic sentence is valid for the whole discipline: There are differences between subdomains in materials science that make this process highly dependent on domain experts.

A material may be expressed in a multitude of partially overlapping definitions: as a single material, a single sample, a family, or a class.
Alternative approaches to naming exist, including the use of commercial names or sample designations, often arbitrarily chosen by researchers. Chemical formulas can be presented as stoichiometric expressions within these three categories, with possibilities such as Y-111 or (A, B) C1 D2. Examples of standard names include Oxygen or Magnesium Diboride, while some adopt abbreviated forms like Yttrium Barium Copper Oxide (YBCO). These varied strategies provide a range of options for nomenclature within the domains of chemistry and materials science, while the conventional method primarily involves the use of chemical formulas.

A class of materials refers to a broader category that shares certain fundamental characteristics or properties. Materials in the same class might be characterised by common structural features, electronic configurations, or other common traits that make them suitable for investigation. 
A family usually implies a more specific grouping that shares a closer genetic or compositional relationship. A set of materials with similar chemical compositions, crystal structures, or electronic configurations. 
A sample is just an arbitrary name chosen by the researchers which can overlap any of the previous definitions. 
To make things more complicated, all these definitions are not strictly enforced and may fluctuate from one laboratory to another. 
In many papers, the authors talk about "high-Tc cuprates" referring to materials in the class of cuprates that also have high-Tc. Such a definition is fallacious and not robust, as it does not clearly describe which materials are included: \textit{"How high should be Tc to be considered 'high'?"}.

Manual curation remains a necessary component, but it does not guarantee complete elimination of errors. As reported by~\cite{sommer20223dsc}, their investigation identified over 10,000 duplicated records within the SuperCon dataset at the time of their study, revealing instances where certain properties were inadequately populated. 
This underscores the inherent challenges and limitations associated with relying solely on manual efforts for data curation, emphasising the imperative to adopt more robust and automated strategies to enhance the accuracy and comprehensiveness of the SuperCon dataset.
The necessity of a curation-centric user interface and tools that allow for the reduction of the manual bottleneck to a few minor actions, leaving to the automation of the tedious task of identifying main data in scientific publications.


\section{Problem definition}

Looking at the current situation, NIMS is facing multiple challenges in maintaining SuperCon: a) updating the database is becoming more expensive and b) it is difficult to steer it to accommodate the extraction of different properties. 
First, collaboration with domain experts becomes imperative because they provide both validation and guidance. 
This collaborative effort is challenging due to different work methodologies; however, it facilitates the gathering of methodologies that can be shared between disciplines, fostering a comprehensive and cohesive solution.
The construction of an automatic process is needed to accelerate the extraction of information from scientific papers. In this dissertation, we discuss our work including the main contributions. 

\section{Contributions}
In the following section we discuss the various contribution of this dissertation, highlighting their main relevancy. 

\subsection{Extraction from full-text body of PDF documents}
% PDF document extraction
One pivotal facet of the problem involves the exploration of extracting information from PDF documents, a novel approach not previously employed in related works that predominantly relied on web scraping or through publisher API.
While web scraping is often not allowed, or very limited to published information (e.g., abstract), API access is often hindered by the necessity of agreements with publishers, making the gathered content difficult to share and challenging to replicate.
The increasing abundance of open-access literature~\cite{laakso2011the} together with the large widespread of PDF format in scientific publication require to develop means to extract structured information that can be exploited in a large set of applications. 
Furthermore, data extraction from PDF documents allow access to a variety of information, full-text body, tables and figures and reduce the human necessity in transforming such information in machine-readable databases.

In our work, we focus on the full text body, which, differently from other approaches~\cite{yamaguchi-etal-2020-sc}, is characterised by the frequent presence of experimental results in related works, thus improving the potential for recall compared to extraction solely from abstracts. 
However, it is acknowledged that abstracts, being synthetic and possessing a simpler structure, also play a role in data extraction.

\subsection{Extraction of experimental data}

The scope of this work is to create databases of experimental data that are reported in the scientific literature. 
The structure to represent such information can be complex and depend strongly on each sub-domain, which has its own characteristics and conventions. 
However, we can simplify the structure of the experimental data using three main information: material expressions, properties, and conditions. 
Materials expression is a relatively loose concept that strongly depends on the domain of application. On the other hand, properties and conditions, can be decomposed down to expressions of measurements of physical units. 

\subsubsection{Identification of complex material sequences}
The field of materials science encompasses various disciplines, including chemistry, physics, and engineering. 
Material expressions within this field are typically lengthy and intricate sequences of characters. Materials themselves can be represented by chemical formulas, which may also include additional information such as the material's shape (e.g., crystal, single crystal, wire) and doping details (e.g., 2\% Zn-doped). It is common for scientists to define samples and assign arbitrary names to them. Chemical formulas often involve substitutions in either the amount of an element (e.g., La x Fe 1-x O 7) or the element itself (e.g., RE x Fe 1-x O 7 with RE = La, Cu) to represent multiple rare earth compounds. Frequently, samples are denoted by their doping variable amount (e.g., x = 0.1).

Identification and extraction of these entities is extremely challenging. 
We use machine learning (ML) techniques to implement the named entity recognition (NER) system. This approach offers several advantages such as context awareness, increased robustness to noise, and better generalisation to unseen examples compared to rule-based methods. Additionally, we discuss evaluation of three different architectures: Conditional Random Fields (CRF), Recurrent Neural Networks (RNN), and BERT-based models.

Differently from other works we apply cascade approach where the original text is first processed and the main entities are extracted: materials and classes, properties (pressure, Tc), conditions (measurement methods). 
Then, material entities are further processed by a specialised material parser, where different modifiers such as doping, shape, substrate, chemical formula, and name are identified. 
Our approach also applies positive sampling; at training, we boost the recall of entities by providing examples with valid entities. This results in a gain of F1 score of a few percentage scores. 
In the end, we incorporate the use of characteristics consisting of text patterns (such as uppercase and lowercase) and layout details (such as superscript, subscript, bold, and italic) that are exclusively extracted from the PDF structure.

\subsubsection{Extraction of properties and conditions as measurements of physical quantities}

Properties and conditions within the realm of science and engineering are often expressed through measurements of physical quantities. These measurements serve as a universal language, providing a standardised way to convey information about various aspects of the physical world. Whether it's the length of an object, the temperature of a substance, or the intensity of a force, these measurements encapsulate essential characteristics that form the basis of scientific understanding and technological advancements.
The measurement of physical quantities forms the backbone of scientific inquiry, fostering a collective language that transcends boundaries and enables collaborative efforts in the pursuit of knowledge and innovation.

The use of distinct tools for identifying materials and properties enhances flexibility in handling and interpreting information, paving the way for broader support and the integration of additional data in the future. By employing specialized tools for material identification and property analysis, researchers and professionals can adapt to evolving research needs and technological advancements. This modular approach not only facilitates the incorporation of new measurement techniques but also encourages interdisciplinary collaboration.

While measurements are, in appearance, consolidated information, their treatment is challenging: measurement units are often used differently in different disciplines, different systems (miles instead of meters, ATM instead of Pascal) of unit may be used because they are easier to understand for humans, but creating great confusion for machines (one of the most famous event in history was the Mars Climate Orbiter explosion). 

In this contribution, we grobid-quantities, focused on physical quantities and measurement identification and normalisation. At the time of development, grobid-quantities was the first ML-based, open-source project aiming to cover multiple disciplines (from biology to materials science) and system of units (SI base, SI extended, imperial, etc.). 


\subsection{SuperMat: a linked annotated dataset of superconductors research papers}
% SuperMat
A critical gap identified in existing resources is the absence of datasets consolidating relevant entities such as materials, conditions, properties, and their interrelations. 
Currently, no dataset offers a centralised solution for machine learning models, considering the intricate and inconsistent terminology prevalent in this domain. 
To rectify this deficiency, constructing a dataset is deemed imperative, forming the foundation for a methodology that involves iterative data collection and correction. This dissertation contributes to this endeavour by expanding the data set for superconductor materials. 
The inclusion of measurement methods, which enable the selection of experimental properties, and the incorporation of applied pressure, an underexplored area, holds the potential to pave the way for groundbreaking advancements soon.


\subsection{SuperCon~\textsuperscript{2}}

The grobid-superconductors extraction tool marks a significant advancement in the efficiency and scale of data extraction, particularly evident in the creation of the SuperCon2 database. 
This TDM process has proven to be remarkably efficient, taking only a few days to achieve what would have required years of manual effort. 
Originally, the SuperCon database, cultivated over two decades, encompassed approximately 33,000 elements. grobid-superconductors has burgeoned to a staggering 40,324 records, demonstrating a substantial increase in the volume of data processed. 
The SuperCon2 database serves as an invaluable staging ground for the automatic extraction, acting as an intermediary step before integration into the original SuperCon database. 
The tool's proficiency is evident in its ability to not only match the scope of the original database but also exceed it, capturing intricate details such as applied pressure and measurement methods. 
Once curated and validated, the data within SuperCon2 can seamlessly enrich the SuperCon database, ensuring a continuous evolution in the accessibility and depth of the stored scientific information.

\subsection{Curation workflow and interface}

Validating the automatically collected data poses significant challenges due to the potential inaccuracies and errors inherent in automated processes. 
To address this, we have designed a robust solution that features a staging area (SuperCon2) that is accessed through a curation workflow with a user-friendly interface. 
The interface offers functions to control the content of the database records, triggering the state transition to the underlying workflow. 
Requirements form domain experts demand that the underlying data is persisted and the modification can be traced back to the original record. For this reason, our workflow generate new record at each modification maintaining a history which can be inspected at a later time. 
In addition, our system provides an enhanced visualisation of the original PDF document, allowing users to inspect and cross-reference the data efficiently. 
Furthermore, to continually improve the automated data collection process, our solution incorporates machine learning (ML) capabilities. 
Specifically, the system accumulates training data based on corrections made during the validation process, ensuring a continuous refinement and optimisation of the automated data collection through iterative ML training cycles.
In this work we experimented and demonstrated that our workflow and interface can boost the curation recall of missed information from 45\% to 92\% and the F1-score from 52\% to 92\%. 