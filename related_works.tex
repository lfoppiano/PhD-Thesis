
This chapter is divided into different contributions and aspects of this research, organised from basic to specific and temporary. 
This section begins to illustrate the related work to properties and measurement extraction. We then discuss the relative work in materials science. 

\section{Machine Learning}

\cite{stanev_machine_2017} used data from SuperCon in combination with ICSD and AFLOW. 
Only a small subset of materials in SuperCon overlap with those in the ICSD: about 800 with finite Tc and <600 are contained within AFLOW. 
The AFLOW Online Repositories contain calculated properties, but the DFT results have been extensively validated with observed properties. 
The work uses this subset of materials to incorporate structural/chemical information and electronic properties from the AFLOW Online Repositories into their models. 
They predicted about 2000 compounds as candidate superconductors, with the vast majority being cuprates or compounds containing copper and oxygen. 
There were also some materials related to iron-based superconductors, as well as 35 members that were not obviously connected to any high-temperature superconducting families.


\section{Deep Learning}

\subsection{Recurrent Neural Networks}
\cite{lample2016neural} propose a new neural architecture for NER with an architecture based on a bi-directional set of LSTM chains. 
Bi-directional indicates that the text is computed from left to right (forward LSTM) and from right to left (backward LSTM). 
Such architecture can incorporate information that is usually captured by hand-crafted features, and/or gazetteers. 
Such architecture takes in input two types of word representation: character-based word representation, calculated from the supervised corpus and unsupervised word representation, which is learned from unsupervised corpora. 
Unsupervised word representations discussed were word2vec~\cite{mikolov2013efficient} but they can be used also more recent ones: fasText~\cite{joulin2016fasttext}, GloVe~\cite{pennington2014glove}.

\cite{peters2018deep} Propose a new type of deep contextualised word representation that models a) complex characteristics of word use, and b) uses words across linguistic contexts (polysemy). 
The solution proposed provides word representation calculated from the entire sequence. The network is composed of two bidirectional LSTM layers of dimensions 4096 x 512. 
The two layers work complementary: one that computes left-to-right the probability of the next token based on the previous, and another right-to-left.
This work also observes several important aspects. A) Instead of outputting the layer from the last layer, averaging with a coefficient all weights from all layers improves results in downstream tasks. B) In such a network, lower levels capture information about aspects of syntax while higher levels capture context-dependent aspects (semantic). C) Including the ELMo embeddings in the output as well as in the input improves tasks that use the attention layer after the RNN, while it does not help for other types of tasks.  


\subsection{Transformers}

(Devlin et al, 2019) \cite{devlin2018bert} introduce BERT (Bidirectional Encoder Representation Transformer). 
While pre-trained language models have been discussed already in previous work~\cite{radford2018improving}, and can be exploited in two main ways: features-based or fine-tuning. 
BERT aims to improve the fine-tuning approach and to solve the limitation of existing left-to-right language models, where each token can attend only to previous tokens in the self-attention layers~\cite{vaswani2017attention} of the Transformers architecture. 
BERT introduces an MLM (Masked Language Model) pre-training objective, where it randomly masks tokens in training examples and tries to predict them using the full context. This enables to use of both left and right context and pre-trains a fully Bidirectional Transformer.
Using the original transformer architecture~\cite{vaswani2017attention}, they define BERT\textsubscript{BASE} with L=12, H=768 and A=12 (110M parameters) and BERT\textsubscript{LARGE} as L=24, H=1024 and A=16 (340M parameters) with (L=layers, H=hidden layers, and A=attention heads).
The authors define the input as a sequence which can be either a natural sentence, a paragraph, or a pair of sentences. They use a special separator [CLS] to start the sequence and to convey the result of the final hidden state. For separating two sentences from a pair, they encode the separator with [SEP] and they provide sentence embedding which states whether a token belongs to sentence A or B. 
Each token is encoded using position embeddings calculated using WordPiece using a dictionary of 30000 tokens vocabulary. 
The input representation for each token is represented by the sum of the vocabulary token, the sentence token and the position embeddings. 
The authors defined two training objectives: Masked LM (MLM) and Next Sentence Prediction (NSP). 
The MLM is prepared by substituting 15\% of the token's positions at random: 80\% of the time with [MASK], 10\% of the time with a random token, and 10\% of the time do not substitute. The reason is to provide the model with enough randomness so that the model does not learn always to replace tokens with [MASK]. 
The NSP aims to identify whether a sentence is following another sentence. This is justified by the need to perform tasks such as question-answering where relations between sentences are identified. 
BERT outperform all the present architectures on most of the NLP tasks and benchmarks such as GLUE, MNLI and SQUAD. 


\cite{liu2019roberta} reproduces the results obtained in BERT, revises the original architecture and proposes RoBERTa (Robust Optimised BERT Architecture), a revised BERT implementation which outperforms BERT on downstream tasks such as GLUE, RACE and SQUAD benchmarks. 
The main modifications are summarised as follows. Instead of masking tokens statically only once during preprocessing when the data is duplicated 10 times, they apply different masking at each instance of the same example. Dynamic masking helps for less than 1 point percentage.
They suggest that changing the way the data is provided in input in addition to removing the NSP loss objective, improves the performance. 
In particular, providing full sentences from the same or different documents without NSP matches the original implementation of providing a pair of segments with NSP. 
Furthermore, results get even better by providing only sentences from the same documents and removing the NSP.
The NSP removal was already discussed in recent work (Lample and Conneau, 2019)~\cite{lample2019cross}.
Lastly, they propose training for a longer time and supplying more data can improve the results: the original BERT was trained on 1M steps with a batch size of 250, while the RoBERTa was trained for fewer steps 31000 using a much larger batch size of 8000 examples. This setting requires a more scalable approach because the GPU memory required is much higher. 
With these changes, RoBERTa is able to outperform BERT on most of the language understanding benchmarks such as SQUAD, GLUE and RACE. 

\cite{Beltagy2019SciBERT} propose a scientific version of BERT, called SciBERT. 
SciBERT was trained on a random sample of 1.4M papers from Semantic Scholar (the dataset was not published): 18\% from computer science and 82\% from the biomedical domain. SciBERT was evaluated against BERT and was outperforming BERT. 
Differently from BioBERT~\cite{lee2019biobert}, SciBERT was trained using a different tokenizer (SentencePiece instead of WordPiece). Most importantly, the tokenizer was re-trained from scratch using scientific text. This 
Another interesting aspect is that BioBERT was trained from a model initialised with the weight from BERT, and overall, was trained for a longer time and on more data. Nevertheless, the scarce coverage of the BERT tokeniser on scientific data penalised the performances on various downstream tasks.


\cite{yasunaga2020linkbert} introduces a new approach in pre-training a BERT model in two flavours: LinBERT and BioLinkBERT on the general text and biomedical text, respectively.
The authors introduce a novel approach which includes text from linked documents and an additional objective function is to classify the type of link.
The pre-training data is aggregated as follows: a) Each example is structured as a pair of text sequences as in the original BERT. b) The pairs are aggregated by selecting the second segment (when available) as follows: randomly, from the same document or from a related document. For the linkBERT this only works for the Wikipedia corpus, exploiting the links between Wikipedia pages. For bioLinkBERT, the authors pick sequences from related documents in the citation graph. 
The authors replace the NSP objective with the DRP (Document Relation Prediction) which aims to classify the type of relation between the two segments, giving three possible classes: contiguous, random, and linked.  
Evaluation metrics calculated on QA (extractive question answering), GLUE and SQUAD, outperform BERT. BioLinkBERT outperforms PubMedBERT~\cite{gu2020pubmedbert} on most of the NLP tasks. 


\cite{gu2020pubmedbert} pre-trained a biomedical-based model called PubMedBERT and evaluated using a newly assembled dataset called BLURB (Biomedical Language Understanding \& Reasoning Benchmark). 
The vocabulary was trained using a BPE (Byte-Pair Encoding) approach with a length of 30522, and trained on text from PubMed abstracts: 14 million abstracts, 3.2b words. 
The fact they only use abstract could be a major limitation on the type of information that are used in the pre-training. 
They are the first that consider the concept of "in-domain" (in their case, biomedical text) and "out-of-domain" (everything else). 
Their strong claim is that domain-specific pre-training from scratch can be superior to mixed-domain pretraining which is in contrast with~\cite{hong2022ScholarBERT}. 
From the perspective of vocabulary, it's indeed demonstrated in comparing scores from BIOBERT and SCIBERT that vocabulary specificity improves the performances on tasks applied to scientific text. 

\cite{su2022investigation} tested biomedical-trained BERT models on relation extraction in biomedical data. They demonstrate that downstream tasks on overlapping data that were used in pre-training bias the results for RE. In classification, they demonstrate that using only the information in the "CLS" token can be improved by adding complementary information from other layers. 

\cite{hong2022ScholarBERT} demonstrate that training with multi-disciplinary corpus gives better results than a transformer based on a domain-specific dataset. In addition, they also found that larger models do not always perform better. However, these results might be due to other issues they have introduced with their pre-training process. 
Previous work from~\cite{lample2019cross} has suggested a similar concept applied to translation tasks, where including cross-training information and in particular the addition of data in different languages boost not only the performances in translation but also help the performance in non-translation tasks. 

\cite{huang2020batterybert} is another specialised set of specialised BERT models focusing on Battery-related scientific articles. In this work the authors experiment with different approaches for fine-tuning, continuing a generic BERT training (batterybert), a SciBERT training (batteryscibert) or training from scratch a new model based only on text from battery research (batteryonlybert). The main differences are provided by the different vocabularies, where batterybert and batteryscibert are constrained by the original vocabulary, batteryonlybert vocabulary was trained from scratch and could model more accurately the terminology used in the battery research articles. 
The outcome is that on document classification, batteryscibert obtained the higher score. SQuad evaluation (extractive Q\&A dataset) surprisingly showed better scores with batteryBERT. This work demonstrated the importance of having a solid base with general text as well as scientific information to provide a model that can generalise and adapt to multiple sets of situations where it could be used. 

\cite{guo2021automated} proposes a two-stage deep learning framework for extracting chemical formulas and applying a role labelling in the chemical reaction description. 
Following a common schema Extract-Link, they first extract all mentions to chemical compounds and, subsequently, they label each compound with their role in the reaction description. 
They introduce a chemical-focusing pre-trained BERT flavour followed by a task-adaptive encoder (ChemRxnBERT) that provides a role labelling method. 
The authors created a small dataset for fine-tuning chemical reactions annotations, and obtained from chemistry journals where each article text filtered to just the paragraph referring to a chemical reaction. 
The role labelling was implemented in cascade by providing the main material between special tokens ([P], [\\P]) which are then linked to the other extracted entities. Since the sequence is shallow, it can only link one material with the rest of the reaction. So multiple passages are necessary if multiple products are present in the reaction paragraph.  

\subsection{Transformers fine-tuning experiments}

~\cite{dominic2018revisiting} revisit the hyperparameters when training deep learning networks, focusing, in particular on batch size and learning rate. The authors discuss a different approach for convolutional neural networks and report that mini-batch size of size as low as m = 2, can provide better scores for medium-low datasets. For larger datasets, however, the batch size should not be kept greater than m = 32. 
Batch size is an important hyperparameter, while it should be kept large for pre-training~\cite{liu2019roberta}, evidence indicates that higher performances will be obtained by smaller values for fine-tuning. 

\cite{smith2017dont} suggest a more performing approach could be to increase the batch size instead of decaying the learning rate while fine-tuning. 

\cite{popel2018training} provides suggestions for improving the fine-tuning applied to translation tasks. Their finding can be summarised as follows: a) multi-GPU scaling is more effective than batch-size scaling on a single GPU, and moreover is better to run sequential processes on multi-GPU than parallel processes on a single GPU. b) Displaying the full learning curve is more effective than using the "early stop". c) comparing different datasets requires enough training time. Generally, large datasets converge with more time. 
d) Big models should be preferred if training for several days. e) sequence length should be kept as low as possible, depending on the dataset, this will allow a larger batch size. f) batch size should be kept as high as possible (this was also confirmed by \cite{liu2019roberta}

\cite{mehrafarin2022on} stress the importance of the data size when fine-tuning and found that larger training affects mainly higher layers. In particular, the number of training steps is more relevant than the diversity of the dataset (??). 
The authors compare a pre-trained BERT (baseline) with the corresponding fine-tuned models using 5 GLUE datasets of different sizes and other characteristics.
To measure the amount of linguistic knowledge retained by the model, they designed a set of probes: bigram shift, object number, etc. They run the probes with frozen models and measure that up to layer 6 each subsequent layer retains more linguistic knowledge than the previous one. However, for fine-tuned models on large datasets, such information is lost as we move to higher layers. 

\cite{hoffer2017train} suggests that adjusting the learning rate and batch normalisation can reduce the generalisation gap (comparison between training error and validation error). Generalisation keeps improving for a longer time even without any observable change in the training or validation errors. Large or small batches generalise the same if the number of iterations are adapted. 
Finally, they claim that the change in the generalisation gap depends more on the number of updates rather than the batch size. 

% \section{Named Entities Recognition}

\section{Extraction of quantified properties and measurements from scientific paper} 
NER of physical measurement is a fundamental application in scientific text and relevant in other disciplines, including humanistic or social science. 

Attempts to extract measurements from text have been made using many approaches. At the time of this contribution was made we identified the following related work. 

~\cite{aras2014applications} built a tool for quantities extraction based on Apache UIMA (Unstructured Information Management Architecture) in combination with pattern matching which is circling a Finite State Automata (FSA). 
The authors claim that UIMA and the FSA are faster and better suited for processing a large quantity of text.
Their work supports multiple constructs: values, intervals, and enumerations. The units are normalised toward the International System of Standard (SI). 
The units are loaded from a configuration file, which includes the normalised format. In the same work, they also combine keyword extraction and text segmentation which are interesting challenges in the analysis of patent text. 
The authors used Quantalyze\footnote{\url{https://www.quantalyze.com/}}, a commercial tool built to process patents, for comparison. They reported Quantlyze had limited unit support. 

\cite{maiya2015mining} propose a search engine called MQSearch. Their work proposes a rule-base extractor for quantities and units including the measured object. The extractor models each measurement using the 5-tuple: (sign, number, error, scientific notation, units) where only numbers and units are mandatory. 
The data flow is comprised of four phases: pre-processing to mitigate noisy and incorrect character extraction. 
Then, to recognise units they expanded an ontology of units from the OBO Foundry with additional information from external sources and defined one associated rule for each unit. The ontology is exploited to recognise the measured object. 
Quantities are extracted similarly with a set of regular expressions. 
The last phase is post-processing which is used to discard possible wrong or invalid values. 
They also present a search engine based on SoLR that allows search using the extracted units, values and measured objects. 

Another group built an extractor for patents~\cite{agatonovic2008large} using GATE (General Architecture for Text Engineering). 
Although GATE provides plugins for machine learning, they implemented their extraction using a lookup to a gazetteer and a database containing the transformation rules between one unit to another. 
The gazetteer was built from a set of units published by the GNU (Gnu's not Unix) Foundation and that comprises a set of 30000 units. In addition, they also recognise references within the text, and patent information (e.g. patent number, country code). 
They built the annotation rules using JAPE language which is structured as a matching rule and action to perform. 
%The authors evaluate their tool on two aspects: throughput and accuracy. They created a Gold Standard corpus of 51 documents, reporting the accuracy of 

While most of the works are based on English, there are also some contributions focusing on specific languages: \cite{hetsevich2014processing} investigated issues applied to Russian-derived languages (Russian and Belarusian). 
The authors make a consistent analysis of the challenges with the Russian-derived languages. 
They propose a solution based on finite-state automation with 350 graphs that are running on top of the linguistic processor NooJ. 
The grammar covers three out of the six grammar constructs (genitive, accusative, and nominative). 
The evaluation was made on a mixed corpus of 100,000 words and resulted in an F1-score of 82\%. 

\cite{berrahou2013extract} combine the use of an Ontological and
Terminological Resource (OTR) with the use of ML.
The OTR is used as a reference and can be updated with new units that are not extracted correctly or completely. 
Their aim is to use a modified string-matching function to extract measurements and units. Thus, the main problem they try to solve is to  reduce the space of search using the string matching functions. 
They propose a two-step process. 
First, they use a classifier to determine whether a sentence contains a unit or not. The classifier is built on a bag of words where the words are counted using three methods: TF, TF/IDF and BM25. The model is then built with three different algorithms which are then compared: Naive Bayes, a decision three (J48), and Support vector machines (SVM) and DMNB.
In the second step, they match potential candidates in the OTR and use the string function to select the item that is more likely to represent the extracted one. Based on the similarity, they defined two thresholds which they can consider as a variant of an existing unit or a new unit or to enrich the OTR and improve the future recognition.

\cite{kang_extracting_2013} describe another rule-base solution where they outperform the ML-based respective system. 
Their goal is to extract targeted information from laboratory test results of diagnostic devices examined by  the U.S. Food and Drug Administration (FDA). 
The authors developed a symbolic information extraction (SIE) system for extracting four types of entities: analytes (substance considered), specimens (where the analyte is measured from), units of measures of the analyte, and detection limits of the diagnostic device in the exam.
The SIE is based on a combination of rules and dictionaries. First, the candidates are extracted and then ranked so that the most plausible are selected from the set. The units of measures targeted in this work are only a subset of all the units and specific to this particular sub-domain in biology. 
They evaluated their SIE against three probabilistic learning approaches: CRF, SVM, and HMM. The SIE outperform the ML-based models, except for the unit of measures where the CRF obtained the best scores. 

More related to materials science \cite{dieb2015framework} proposes to integrate relevant units of measurement when composing a small dataset (392 sentences) for nanocrystal device development. 
The dataset focuses on identifying several properties including the property value and units that are common in the domain. 
This work also leverages ML using a set of CRF engines focusing only on a few entity types applied in cascade. 

~\cite{hundman2017measurement} describes an integrated measurement extraction that focuses on scientific literature in earth science. 
They developed a novel method that extracts context-related information to measurements. Instead of developing their own measurement extraction tool, they evaluated three options for reuse. Quantalyze showed low recall in both extracted measurement and measured object and had technical limitations in integration, such as a lack of REST API. ~\cite{agatonovic2008large}, discussed previously, was discarded because of the need for the maintenance of rules. 
Therefore they chose our tool, Grobid-quantities which had the advantage of requiring only labelled data and providing acceptable results in terms of precision, recall and f-score. 

After the publication of our contribution~\cite{foppiano2019quantities}, other works have been published, often focusing on a limited scope or specific language. 

\cite{petersen2021geoquantities} describes a system that aims to extract MAR (Mass Accumulation Rate) information on geographical locations in marine science PDF articles. Since these types of measurements are complex and only applied in their specific domain, it is not surprising that the original version of grobid-quantities does not support them. However, the fact that grobid-quantities is open source and actively developed, they successfully trained a version of grobid-quantities with data that allow the support of MARs. 


% While \cite{ning2022ameta} focuses on the temporal aspect quantities extracted from news feeds. Not relevant, developed later on.

\cite{epp2021stereo} describe a framework for extracting results from documents. They extract three main pieces of information: statistical results, condition information and topics from scientific papers. Although they aim to document following the American Psychological Association (APA) guidelines, there are small variations in many cases. 
Their system exploits a flexible wrapper induction approach for the extraction of statistics and conditions, which is a Grammar-based condition extraction (GBCE) by inferring a set of rules from a subset of papers and applying them to the rest of the dataset. The experiment topics are extracted using an adaptation of the unsupervised Attention-based Aspect Extraction (ABAE) approach. 
The authors use the CORD-19 dataset, 500 documents (0.25\%) are used for learning the rules with the GBCE and extracting information with 99\% precision for APA-compliant documents and 95\% precision for non-APA-compliant documents. 
The rule-based nature is strongly dictated by the uniform structure of the writing style, and possibly to a limited amount of units and measurements that are expected to be extracted. 

\cite{hao2016} extracts numeric lab test expressions in clinical trial eligibility criteria texts. 
They encode information using the specification language TimeML. The system presented Valx extracts: a) numeric values and units using regular expressions, b) using a hybrid approach with a knowledge base, they extract variables referred to as quantities. They cover different text representation (e.g. BMI, Body Mass Index, etc..). Valx support the association of multiple values to the same variable, for example, "40 and 60 years" associate both values to "years". Their tool supports normalisation to convert conventional units to international units. 

\cite{roy2015reasoning} describe an approach of formalisation of quantities and measurements aiming to extract information from free text and infer complex reasoning including entities being measured and their quantity. 
The authors introduce the QVR (Quantity Value Representation) with three constituents: Value (which includes values and ranges), units (which characterise the value), and change (which indicates whether there is a modification in the value, e.g. increases). 
In our work, we do not consider the modification of values, because we focus on finite values. 
The extraction of quantities is performed using a Semi-CRF and a bank of classifiers. Both methods scores around 80\% F1-Score. 
They use four types of features that are calculated for each token (and a window including the three previous and following tokens): a) classification using a lexicon to identify whether it appears as a number, unit, etc.., b) determining whether it contains a digit, all digits, etc.., c) POS (Part of Speech). Units are extracted assuming they are adjoined to the numeric values. 
This work reserves a detailed discussion on quantities entailment, given a quantity-value, and a text it is a 3-ways decision problem: a) entail: when the quantity-value is supported in the context, b) contradicts when the quantity-value is not supported by the context, instead, the Q-V is different, c) no relation. 
The Quantity entailment aims to clarify whether units are confirmed by the context and if the way they are described can be defined as equivalent (e.g. 2 couples, 4 people). The quantity entailment is relevant for scientific text comprehension, especially for the description of experimental results in a very concise way containing multiple relative comparisons within the same paragraph, which makes it hard to understand. 

\cite{taha2021identifying, ho2021qute} focuses on tables.
\cite{ho2021qute} describe a comprehensive query system where they leverage previously extracted quantities, units and context, to answer questions containing conditions on measurements. 
Their data model is described as triples of the form (entity, quantity, context) where the context gives more information and proof to quantity and entity. 
The triplet is computed offline in several steps: first, the quantity is extracted based on \cite{roy2015reasoning}, we have described previously. 
Entities and quantities are referred to using the Yago knowledge graph and different columns are related to their previous work~\cite{ho2021extracting} and the extracted data is then used for creating an index with conceptualised quantities. 
This idea is particularly relevant to our grobid-quantity work because there are very few works at the moment that leverage the extracted property, quantities and through an international system normalisation allow the search by values and interval with the normalisation helping to increase the recall on quantities not in normalised units. 

\cite{taha2021identifying} assumes that the table columns are already identified and focus on covering unconventional unit names (e.g. LTS as Liters), considering that the columns should have the same normalised unit. The authors evaluate and compare their system (PUC) on currencies, data storage, length, mass, and volumes. For example, Grobid-quantities do not support well currencies because they hardly appear in scientific papers. At the moment of writing Grobid-quantities does not support extraction from tables, however since there is a plan to improve the table recognition in the Grobid library, this future work could be implemented within the same framework. 


\section{Text and data mining in materials science}

(Court and Cole)~\cite{court2018auto} propose a method to extract curie and Neel temperatures from scientific papers. 
The authors considered the use of ChemDataExtractor for this task but realised that with such type of scientific text, the rule-base nature was not well suited and resulted in lower performances such as precision and recall. 
Therefore, they extended ChemDataExtractor with a semi-supervised relationship extraction algorithm. 
Using a modified version of the Snowball algorithm, which uses a semi-automatic approach to learn typical pattern on which such relations exists. 
The pattern learning can be summarised as follows: a) first the sentences are identified, b) the CEM (Chemical Entities Mentions) are identified, and the sentence is characterised by the number of mentions that are extracted. 
Furthermore, c) the sentence is tokenised, and based on the previously extracted CEM, is structured as follows: (prefix, e\textsubscript{1}, middle\textsubscript{1}, e\textsubscript{2}, etc... e\textsubscript{m}, suffix) with e\textsubscript{1..m} the extracted entities (or mentions).
The patterns are then clustered by distance similarity after having transformed them into vectors using the Term Document Frequency (TDF) model.
After the patterns are collected, the new relationship can be discovered by combining the clustering with all pre-generated patterns. 
At each step, they compute a confidence score, which allows the evaluation of whether the pattern should be kept or discarded. 
The authors claim they extracted 39822 records, consisting of 11340 Néel and 28482 Curie temperature records. 

\cite{kononova2019text} describes a text mining system for extracting and segmenting synthesis recipes from scientific articles. 
They retrieve the articles by web scraping and identify the synthesis paragraphs by classification. Then they apply NER on the identified paragraph using RNN (BidLSTM + CRF) which identifies the different constituents of the recipe and they extract the synthesis in terms of precursor, operation, and conditions. They extract 19,488 solid-state synthesis reactions.

(Court and Cole)~\cite{court2020magnetic} propose another work where they process 74000 web-scraped scientific journal articles and build a database of 20,400 magnetic and superconducting phase transition temperature records and their associated chemical compound names.
The NLP pipeline works as follows: a) targeted journals were processed to extract entities of materials formulas and temperatures from text and tables. b) the entities were then standardised, the formulas were converted to Hill, and the temperatures were converted to Kelvin, and doping was resolved. Ambiguous temperatures were classified and the data was stored in a MongoDB database. 


\cite{venugopal2021looking} proposes a framework to identify common trends in materials-science domains using keyword extraction from abstract and figure captions. Using a combination of LDA and NER they construct a topical map across the literature of glass-related research. 


ML does extract well implicit knowledge from data~\cite{keith2021combining} and could provide new perspective on already existing problems. 
ML in complex domains, such as chemistry, requires high-quality data and does not guarantee that the outcome is due to the provided domain knowledge or some other hidden aspects from the data itself~\cite{keith2021combining}.

\cite{choudhary2023chemnlp} presents ChemNLP, a natural language processing (NLP) library and web app for analyzing materials' chemistry text data.
It utilizes the publicly available arXiv dataset of ~1.8 million scholarly articles collected over 34 years.
Analysis is done on publication trends, author names, taxonomy categories, and word frequencies in titles and abstracts.
An interactive web app was built to search for articles containing specific chemical elements and compounds.
As a demonstration, ChemNLP was applied to identify new superconducting materials by comparing the arXiv dataset to a DFT-based superconductor database.
Machine learning techniques like TF-IDF vectors, t-SNE clustering, and classification algorithms were used to categorize cond-mat arXiv articles with ~80\% accuracy.
ChemNLP provides an open dataset and tools to apply NLP techniques to materials science literature for knowledge discovery.

\cite{park2018text} proposes a text-mining pipeline for extracting Metal-organic frameworks from scientific papers. In this paper, the authors focus on two main properties: surface area (SA) and pore volume (PV) mainly because they are the main properties related to absorption properties of MOPs and are very commonly described in papers and they have a very distinctive signature such as the units used to measure the quantity. 
The source data was obtained in HTML format and, after parsing, cleaning and tokenising the text, the tokens were classified into 5 types using lexicons from the Cambridge structural databases. In addition, they use special keywords to identify correct units used in unorthodox manners. The resulting classified tokens were aggregated to each other (the material was linked to the related properties) with a rule-based algorithm.
On a sample dataset, the algorithm achieves 90\% and 88.8\% accuracy in extracting surface area and pore volume data respectively.
When tested on a larger dataset of 2315 MOF papers, the accuracy drops to 73.2\% for surface area and 85.1\% for pore volume. Errors occur due to complex sentence structures and incorrect MOF name identification.

SC-CoMIcs~\cite{yamaguchi-etal-2020-sc}  describes a new corpus called SC-CoMIcs (SuperConductivity Corpus for Materials Informatics) tailored for text mining of superconducting materials.
The corpus consists of 1,000 manually annotated abstracts related to superconductivity with seven named entity categories: Characterisation, Process, Property, Material, Element, Doping, and Value. Inter-annotator agreement scores were around 75-85\%, similar to other materials science corpora.
Experiments using SciBERT for named entity recognition achieved a 77\% F1 score, comparable to human agreement.
Learning curves indicate the corpus size is mostly sufficient, though some categories could benefit from more data. The corpus was used to build a term search tool based on word vectors, demonstrating its utility for retrieving relevant terms by category. The paper shows the potential of text mining to extract key information from superconductivity literature. The corpus provides a valuable resource to develop more capable natural language processing systems for materials informatics. Limitations include focusing only on abstracts and the specific superconductivity domain. 


\cite{cruse2022text} introduces a nanoparticle-related dataset of 5,154 records extracted from 4.9 million materials science papers containing synthesis recipes and morphological information.
The authors successfully extracted 7,608 experimental and 12,519 characterization paragraphs with compounds, amounts, synthesis actions, sizes, shapes etc.
Limitations include the inability to distinguish targets from other morphologies and the lack of order information for seed-mediated syntheses.


\cite{ghosh2022band} develop an automated approach to extract band gap values and relate them to chemical compounds from paper titles and abstracts.
The authors extended ChemDataExtractor with a new BandGapParser to identify band gap information.
On a sample of 415 papers, the system achieved 51.32\% correct, 36.62\% partially, and 12.04\% incorrect extractions.
Errors are due to incorrect chemical entity extraction, failure to extract all band gap values, and incorrectly relating entities to values.
The approach was applied to 11,939 papers, extracting 10,608 band gap values for 10,292 compounds.
Evaluation is a limitation, done by manual review of extraction rather than comparing to ground truth.

After our work: 

\cite{pranav2023a} propose a material-property extraction for polymers, based on a fine-tuned BERT model on 2.4 million abstracts. They report their NER system to achieve higher results as compared with other Material-based pre-trained BERT models. The properties are connected to materials using a heuristic approach to finding the closer entities.
With their system, they extract more than 300000 records of polymers and properties from the 2.4 millions abstracts. 
The paper, however, does not provide information about data contamination between pre-training and evaluation: we don't know whether the evaluation datasets - PolymerAbstract~\cite{huan2016a} or the training set of 768 abstracts - are overlapping. 
Moreover, is not indicated whether their model can generalise properly, providing out-of-domain information of their evaluation dataset as compared with the training set. 



\cite{mitsui2023automatic} develops a natural language processing system to extract superconductivity information from scientific literature abstracts.
It uses the SC-CoMIcs corpus of 1000 annotated abstracts on superconductivity to train named entity recognition and relation extraction models.
The system extracts material compositions, transition temperatures, doping information, and process details from 48,565 abstracts for a total of over 43,000 superconducting materials and 24,000 transition temperatures that were extracted.
A machine learning model predicts transition temperatures from compositions with mean error of 15K.
The automatically extracted information enables data-driven exploration for superconducting materials design.
Limitations include relying only on abstracts and inability to fully resolve ambiguous entities.




\section{Relation extraction}

\cite{wu2019enriching} discuss R-BERT, an enhanced BERT architecture that incorporate relation information and combine them by concatenating before the activation function. 
This problem is part of the Semeval 2010 task 8 challenge~\cite{hendrickx2019semeval}, and is limited to relation classification considering that a relation has already been identified.
